【python】
1、三目运算符
1 if i%4 == 0 else 0

2、反射机制
count = __import__(imp)
sum = count.my_sum(2, 3)

3、使用raise抛出异常
x = 10
if x > 5:
    raise Exception('x 不能大于 5。x 的值为: {}'.format(x))

4、关键字参数动态表达
    import datetime
    from dateutil.relativedelta import relativedelta
    today = datetime.date.today()
     # 日、月、年
    day = today - relativedelta(**{['days', 'months', 'years'][0]: 1})

5、判断对象是否为string
    print(isinstance('abc',str))


7、pandas和pyspark互转dataframe
# pandas转spark
values = pandas_df.values.tolist()
columns = pandas_df.columns.tolist()
spark_df = spark.createDataFrame(values, columns)

# spark转pandas
pandas_df = spark_df.toPandas()
spark和pandas的datafram



8、rest api认证
    # RESTful API开发中，Authentication（认证）机制的实现通常『非常必要』。
    user_password = '%s:%s' % (es_user, es_psd)
    b64_author = base64.b64encode(user_password.encode())
    author = 'basic ' + b64_author.decode('utf-8')
    headers = {'Content-Type': 'application/json', 'Authorization': author}


【spark】
1、计算分组count的总和
from pyspark.sql import functions as F
total = df2.agg(F.sum('count')).first()[0]

2、数值格式化
concat(format_number(100*count/{1}, 3),'%')
sql3 = "select concat({0}*setp_num + 1,'~',{0}*(setp_num+1)) as bucket,format_number(100*count/{1}, 5) as percent from data2".format(


3、组内排序(最值)
select a.Classid,a.English from
(select Classid,English,row_number() over(partition by Classid order by English desc) as n
from CJ) a where n<=2

4、组内合并
select userId,collect_list(emb) emb_list from temp grop by userId

5、数组拆分多行
select pair[0] as pair_0,pair[1] as pair_1 from temp lateral view explode(seqs_pair(item_seq)) as pair
