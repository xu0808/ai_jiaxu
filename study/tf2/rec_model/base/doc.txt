推荐算法也可以很简单
https://www.zhihu.com/column/c_1330637706267734016

【推荐算法(一)——FM因式分解】
优点：
将二阶交叉特征考虑进来，提高模型的表达能力；
引入隐向量，缓解了数据稀疏带来的参数难训练问题；
模型复杂度保持为线性，并且改进为高阶特征组合时，仍为线性复杂度，有利于上线应用。
缺点：
虽然考虑了特征的交叉，但是表达能力仍然有限，不及深度模型；
同一特征与不同特征组合使用的都是同一隐向量，违反了特征与不同特征组合可发挥不同重要性的事实。

【推荐算法(二)——FFM】
FFM（Field-aware Factorization Machine）是 FM 的改进版
优点：
引入field域的概念，让某一特征与不同特征做交互时，可发挥不同的重要性，提升模型表达能力；
可解释性强，可提供某些特征组合的重要性。
缺点：
复杂度高，不适用于特征数较多的场景。

【推荐算法(三)——Wide&Deep】
需要注意的是，两部分的输入不同：
Wide 部分：Dense Features + Sparse Features（onehot 处理）+  特征组合
Deep 部分：Embeddings (Sparse Features embedding 处理)

优点:
1 结构简单，复杂度低，目前在工业界仍有广泛应用；
2 线性模型与深度模型优势互补，分别提取低阶与高阶特征交互信息，兼顾记忆能力与泛化能力；
3 线性部分为广义线性模型，可灵活替换为其他算法，比如 FM，提升 wide 部分提取信息的能力。
缺点：
1 深度模型可自适应的进行高阶特征交互，但这是隐式的构造特征组合，可解释性差；
2 深度模型仍需要人工特征来提升模型效果，只是需求量没有线性模型大。

【推荐算法(四)——经典模型 DeepFM】
Deep与FM结合的产物，也是Wide&Deep的改进版，只是将LR替换成了FM，提升了模型wide侧提取信息的能力。
与Wide&Deep的异同：
相同点：都是线性模型与深度模型的结合，低阶与高阶特征交互的融合。
不同点：DeepFM两个部分共享输入，而Wide&Deep的wide侧是稀疏输入，deep侧是稠密输入；
       DeepFM无需加入人工特征，可端到端的学习，线上部署更方便，Wide&Deep则需要在输入上加入人工特征提升模型表达能力。

优点：
1 两部分联合训练，无需加入人工特征，更易部署；
2 结构简单，复杂度低，两部分共享输入，共享信息，可更精确的训练学习。
缺点：
1 将类别特征对应的稠密向量拼接作为输入，然后对元素进行两两交叉。
这样导致模型无法意识到域的概念，FM与Deep两部分都不会考虑到域，属于同一个域的元素应该对应同样的计算。

【推荐算法(五)——谷歌经典 Deep&Cross Network】
简称DCN，是基于Wide&Deep的改进版，把wide侧的LR换成了cross layer，
可显式的构造有限阶特征组合，并且具有较低的复杂度。
优点：
1 引入cross layer显示的构造有限阶特征组合，无需特征工程，可端到端训练；
2 cross layer具有线性复杂度，可累加多层构造高阶特征交互，
  因为类似残差连接的计算方式，累加多层也不会产生梯度消失问题；
3 跟deepfm相同，两个分支共享输入，可更精确的训练学习。
缺点：
1. DCN不会考虑域的概念，属于同一特征的各个元素应同等对待；


【推荐算法(六)—— xDeepFM】
xDeepFM是Wide&Deep的改进版，在此基础上添加了CIN层显式的构造有限阶特征组合。
xDeepFM虽然名字跟DeepFM类似，但是两者相关性不大，DCN才是它的近亲。

优点：
使用vector-wise的方式，通过特征的元素积来进行特征交互，将一个特征域的元素整体考虑，
比bit-wise方式更make sence一些；
缺点：
CIN层的复杂度通常比较大，它并不具有像DCN的cross layer那样线性复杂度，
它的复杂度通常是平方级的，因为需要计算两个特征矩阵中特征的两两交互，这就给模型上线带来压力。
为什么CIN叫压缩感知层？因为每次矩阵W都会将特征两两交互得到的三维矩阵压缩成一维，所以叫做压缩感知。
